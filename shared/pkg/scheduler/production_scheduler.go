package scheduler

import (
	"fmt"
	"log"
	"time"

	"github.com/psantana5/ffmpeg-rtmp/pkg/models"
	"github.com/psantana5/ffmpeg-rtmp/pkg/store"
)

// ExtendedStore defines extended store methods for FSM operations
type ExtendedStore interface {
	store.Store
	TransitionJobState(jobID string, toState models.JobStatus, reason string) (bool, error)
	AssignJobToWorker(jobID, nodeID string) (bool, error)
	CompleteJob(jobID, nodeID string) (bool, error)
	UpdateJobHeartbeat(jobID string) error
	GetJobsInState(state models.JobStatus) ([]*models.Job, error)
	GetOrphanedJobs(workerTimeout time.Duration) ([]*models.Job, error)
	GetTimedOutJobs() ([]*models.Job, error)
}

// ProductionScheduler is a production-grade scheduler with strict FSM and fault tolerance
type ProductionScheduler struct {
	store              store.Store
	config             *SchedulerConfig
	metrics            *SchedulerMetrics
	stopCh             chan struct{}
	schedulingStopCh   chan struct{}
	healthStopCh       chan struct{}
	cleanupStopCh      chan struct{}
}

// SchedulerConfig holds scheduler configuration
type SchedulerConfig struct {
	SchedulingInterval   time.Duration // How often to run job assignment
	HealthCheckInterval  time.Duration // How often to check worker health
	CleanupInterval      time.Duration // How often to run cleanup/recovery
	WorkerTimeout        time.Duration // How long before worker is considered dead
	RetryPolicy          *models.RetryPolicy
	JobTimeout           *models.JobTimeout
}

// DefaultSchedulerConfig returns sensible defaults
func DefaultSchedulerConfig() *SchedulerConfig {
	return &SchedulerConfig{
		SchedulingInterval:   2 * time.Second,
		HealthCheckInterval:  5 * time.Second,
		CleanupInterval:      10 * time.Second,
		WorkerTimeout:        2 * time.Minute,
		RetryPolicy:          models.DefaultRetryPolicy(),
		JobTimeout:           models.DefaultJobTimeout(),
	}
}

// SchedulerMetrics tracks scheduler performance
type SchedulerMetrics struct {
	QueueDepth          int
	AssignmentAttempts  int
	AssignmentSuccesses int
	AssignmentFailures  int
	RetryCount          int
	TimeoutCount        int
	OrphanedJobsFound   int
	WorkerFailures      int
	LastSchedulingRun   time.Time
	LastHealthCheck     time.Time
	LastCleanup         time.Time
}

// NewProductionScheduler creates a production-grade scheduler
func NewProductionScheduler(st store.Store, config *SchedulerConfig) *ProductionScheduler {
	if config == nil {
		config = DefaultSchedulerConfig()
	}

	// Verify store implements ExtendedStore
	if _, ok := st.(ExtendedStore); !ok {
		log.Printf("[Scheduler] Warning: Store does not implement ExtendedStore interface")
	}

	return &ProductionScheduler{
		store:            st,
		config:           config,
		metrics:          &SchedulerMetrics{},
		stopCh:           make(chan struct{}),
		schedulingStopCh: make(chan struct{}),
		healthStopCh:     make(chan struct{}),
		cleanupStopCh:    make(chan struct{}),
	}
}

// Start begins all scheduler loops
func (s *ProductionScheduler) Start() {
	log.Printf("[Scheduler] Starting production scheduler (scheduling: %v, health: %v, cleanup: %v)",
		s.config.SchedulingInterval, s.config.HealthCheckInterval, s.config.CleanupInterval)

	// Start separate loops
	go s.schedulingLoop()
	go s.healthLoop()
	go s.cleanupLoop()
}

// Stop gracefully stops all scheduler loops
func (s *ProductionScheduler) Stop() {
	log.Println("[Scheduler] Stopping production scheduler...")
	close(s.stopCh)

	// Wait for loops to stop (with timeout)
	timeout := time.After(10 * time.Second)
	done := make(chan struct{})

	go func() {
		<-s.schedulingStopCh
		<-s.healthStopCh
		<-s.cleanupStopCh
		close(done)
	}()

	select {
	case <-done:
		log.Println("[Scheduler] All loops stopped gracefully")
	case <-timeout:
		log.Println("[Scheduler] Stop timeout - forcing shutdown")
	}
}

// schedulingLoop handles job assignment to workers
func (s *ProductionScheduler) schedulingLoop() {
	defer close(s.schedulingStopCh)

	ticker := time.NewTicker(s.config.SchedulingInterval)
	defer ticker.Stop()

	log.Println("[Scheduler] Scheduling loop started")

	for {
		select {
		case <-ticker.C:
			s.runSchedulingCycle()
		case <-s.stopCh:
			log.Println("[Scheduler] Scheduling loop stopped")
			return
		}
	}
}

// healthLoop monitors worker health and heartbeats
func (s *ProductionScheduler) healthLoop() {
	defer close(s.healthStopCh)

	ticker := time.NewTicker(s.config.HealthCheckInterval)
	defer ticker.Stop()

	log.Println("[Scheduler] Health loop started")

	for {
		select {
		case <-ticker.C:
			s.runHealthCheck()
		case <-s.stopCh:
			log.Println("[Scheduler] Health loop stopped")
			return
		}
	}
}

// cleanupLoop handles orphan recovery and retries
func (s *ProductionScheduler) cleanupLoop() {
	defer close(s.cleanupStopCh)

	ticker := time.NewTicker(s.config.CleanupInterval)
	defer ticker.Stop()

	log.Println("[Scheduler] Cleanup loop started")

	for {
		select {
		case <-ticker.C:
			s.runCleanupCycle()
		case <-s.stopCh:
			log.Println("[Scheduler] Cleanup loop stopped")
			return
		}
	}
}

// runSchedulingCycle assigns queued jobs to available workers
func (s *ProductionScheduler) runSchedulingCycle() {
	s.metrics.LastSchedulingRun = time.Now()

	// Get queued jobs (priority ordered)
	queuedJobs, err := s.getQueuedJobsPrioritized()
	if err != nil {
		log.Printf("[Scheduler] Error getting queued jobs: %v", err)
		return
	}

	s.metrics.QueueDepth = len(queuedJobs)

	if len(queuedJobs) == 0 {
		return
	}

	// Get ALL workers for capability validation
	allWorkers := s.store.GetAllNodes()
	
	// Get available workers for assignment
	availableWorkers := s.getAvailableWorkers()
	if len(availableWorkers) == 0 && len(allWorkers) == 0 {
		return
	}

	log.Printf("[Scheduler] Scheduling: %d queued jobs, %d available workers",
		len(queuedJobs), len(availableWorkers))

	// Process each queued job
	for _, job := range queuedJobs {
		// First, check if ANY worker in cluster can ever run this job
		canRunAnywhere, clusterReason := ValidateClusterCapabilities(job, allWorkers)
		if !canRunAnywhere {
			// Job cannot be satisfied by cluster - reject immediately
			log.Printf("[Scheduler] Rejecting job %d: %s", job.SequenceNumber, clusterReason)
			s.rejectJob(job, clusterReason)
			continue
		}

		// Job CAN run somewhere, but check if any worker is available now
		if len(availableWorkers) == 0 {
			// No workers available right now, but job is valid - keep in queue
			continue
		}

		// Find compatible available workers
		compatibleWorkers, reason := FindCompatibleWorkers(job, availableWorkers)
		if len(compatibleWorkers) == 0 {
			// No compatible workers available right now, but job is valid - keep in queue
			log.Printf("[Scheduler] Job %d waiting for compatible worker: %s", 
				job.SequenceNumber, reason)
			continue
		}

		// Assign to first compatible worker
		worker := compatibleWorkers[0]
		s.metrics.AssignmentAttempts++

		// Attempt idempotent assignment
		ext, ok := s.store.(ExtendedStore)
		if !ok {
			log.Printf("[Scheduler] Store does not support extended operations")
			continue
		}

		success, err := ext.AssignJobToWorker(job.ID, worker.ID)
		if err != nil {
			log.Printf("[Scheduler] Failed to assign job %s to worker %s: %v",
				job.ID, worker.ID, err)
			s.metrics.AssignmentFailures++
			continue
		}

		if success {
			s.metrics.AssignmentSuccesses++
			log.Printf("[Scheduler] Assigned job %d (queue=%s, priority=%s) to worker %s",
				job.SequenceNumber, job.Queue, job.Priority, worker.Name)
			
			// Remove assigned worker from available list
			availableWorkers = removeWorker(availableWorkers, worker.ID)
		}
		
		// Stop if no more workers available
		if len(availableWorkers) == 0 {
			break
		}
	}
}

// runHealthCheck monitors worker heartbeats and marks dead workers
func (s *ProductionScheduler) runHealthCheck() {
	s.metrics.LastHealthCheck = time.Now()

	// Get all active workers
	workers := s.store.GetAllNodes()
	now := time.Now()
	deadWorkers := []string{}

	for _, worker := range workers {
		if worker.Status == "offline" {
			continue
		}

		timeSinceHeartbeat := now.Sub(worker.LastHeartbeat)
		if timeSinceHeartbeat > s.config.WorkerTimeout {
			log.Printf("[Health] Worker %s (%s) dead - no heartbeat for %v (threshold: %v)",
				worker.Name, worker.ID, timeSinceHeartbeat, s.config.WorkerTimeout)

			// Mark worker offline
			if err := s.store.UpdateNodeStatus(worker.ID, "offline"); err != nil {
				log.Printf("[Health] Failed to mark worker %s offline: %v", worker.ID, err)
			} else {
				deadWorkers = append(deadWorkers, worker.ID)
				s.metrics.WorkerFailures++
			}
		}
	}

	if len(deadWorkers) > 0 {
		log.Printf("[Health] Detected %d dead workers", len(deadWorkers))
	}

	// Check for timed out jobs
	s.checkTimedOutJobs()
}

// runCleanupCycle recovers orphaned jobs and schedules retries
func (s *ProductionScheduler) runCleanupCycle() {
	s.metrics.LastCleanup = time.Now()

	// Find orphaned jobs (on dead workers)
	orphanedJobs, err := s.getOrphanedJobs()
	if err != nil {
		log.Printf("[Cleanup] Error finding orphaned jobs: %v", err)
		return
	}

	if len(orphanedJobs) > 0 {
		log.Printf("[Cleanup] Found %d orphaned jobs", len(orphanedJobs))
		s.metrics.OrphanedJobsFound += len(orphanedJobs)

		for _, job := range orphanedJobs {
			s.recoverOrphanedJob(job)
		}
	}

	// Process retrying jobs (apply backoff, then requeue)
	s.processRetryingJobs()
}

// getOrphanedJobs finds orphaned jobs using store-specific implementation
func (s *ProductionScheduler) getOrphanedJobs() ([]*models.Job, error) {
	// Try store-specific method first
	type orphanedStore interface {
		GetOrphanedJobs(time.Duration) ([]*models.Job, error)
	}

	if os, ok := s.store.(orphanedStore); ok {
		return os.GetOrphanedJobs(s.config.WorkerTimeout)
	}

	// Fallback: manual detection
	return s.findOrphanedJobsManually()
}

// findOrphanedJobsManually uses basic store methods to find orphaned jobs
func (s *ProductionScheduler) findOrphanedJobsManually() ([]*models.Job, error) {
	cutoff := time.Now().Add(-s.config.WorkerTimeout)
	allJobs := s.store.GetAllJobs()
	orphaned := []*models.Job{}

	for _, job := range allJobs {
		if job.Status != models.JobStatusAssigned && job.Status != models.JobStatusRunning {
			continue
		}

		if job.NodeID == "" {
			continue
		}

		node, err := s.store.GetNode(job.NodeID)
		if err != nil || node.Status == "offline" || node.LastHeartbeat.Before(cutoff) {
			orphaned = append(orphaned, job)
		}
	}

	return orphaned, nil
}

// checkTimedOutJobs finds and handles jobs that exceeded their timeout
func (s *ProductionScheduler) checkTimedOutJobs() {
	timedOutJobs, err := s.storeExt().GetTimedOutJobs()
	if err != nil {
		log.Printf("[Health] Error checking timeouts: %v", err)
		return
	}

	for _, job := range timedOutJobs {
		log.Printf("[Health] Job %d timed out (last activity: %v)",
			job.SequenceNumber, job.LastActivityAt)

		s.metrics.TimeoutCount++

		// Transition to TIMED_OUT state
		_, err := s.storeExt().TransitionJobState(
			job.ID,
			models.JobStatusTimedOut,
			fmt.Sprintf("Exceeded timeout threshold (last activity: %v)", job.LastActivityAt),
		)

		if err != nil {
			log.Printf("[Health] Failed to mark job %s as timed out: %v", job.ID, err)
			continue
		}

		// Schedule retry if eligible
		if s.config.RetryPolicy.ShouldRetry(job, "timeout") {
			s.scheduleRetry(job, "timeout")
		} else {
			// Max retries exceeded - mark as failed
			s.storeExt().TransitionJobState(
				job.ID,
				models.JobStatusFailed,
				fmt.Sprintf("Max retries exceeded after timeout (%d/%d)",
					job.RetryCount, s.config.RetryPolicy.MaxRetries),
			)
		}
	}
}

// recoverOrphanedJob handles recovery of a job on a dead worker
func (s *ProductionScheduler) recoverOrphanedJob(job *models.Job) {
	log.Printf("[Cleanup] Recovering orphaned job %d from dead worker %s",
		job.SequenceNumber, job.NodeID)

	// First transition to RETRYING state
	_, err := s.storeExt().TransitionJobState(
		job.ID,
		models.JobStatusRetrying,
		fmt.Sprintf("Worker %s died mid-execution", job.NodeID),
	)

	if err != nil {
		log.Printf("[Cleanup] Failed to transition orphaned job %s: %v", job.ID, err)
		return
	}

	// Schedule retry
	s.scheduleRetry(job, "worker_died")
}

// scheduleRetry schedules a job for retry with exponential backoff
func (s *ProductionScheduler) scheduleRetry(job *models.Job, reason string) {
	// Check retry limit
	if job.RetryCount >= s.config.RetryPolicy.MaxRetries {
		log.Printf("[Cleanup] Job %d exceeded max retries (%d/%d)",
			job.SequenceNumber, job.RetryCount, s.config.RetryPolicy.MaxRetries)

		s.storeExt().TransitionJobState(
			job.ID,
			models.JobStatusFailed,
			fmt.Sprintf("Max retries exceeded (%d/%d)", job.RetryCount, s.config.RetryPolicy.MaxRetries),
		)
		return
	}

	// Calculate backoff
	backoff := s.config.RetryPolicy.CalculateBackoff(job.RetryCount)

	log.Printf("[Cleanup] Scheduling retry for job %d (attempt %d/%d, backoff: %v, reason: %s)",
		job.SequenceNumber, job.RetryCount+1, s.config.RetryPolicy.MaxRetries, backoff, reason)

	// Apply backoff delay
	time.Sleep(backoff)

	// Increment retry count and transition to QUEUED
	// Use RetryJob if available, otherwise manual transition
	err := s.store.RetryJob(job.ID, reason)
	if err != nil {
		log.Printf("[Cleanup] Failed to retry job %s: %v", job.ID, err)
		return
	}

	s.metrics.RetryCount++
	log.Printf("[Cleanup] Job %d re-queued for retry", job.SequenceNumber)
}

// processRetryingJobs handles jobs in RETRYING state
func (s *ProductionScheduler) processRetryingJobs() {
	retryingJobs, err := s.storeExt().GetJobsInState(models.JobStatusRetrying)
	if err != nil {
		log.Printf("[Cleanup] Error getting retrying jobs: %v", err)
		return
	}

	for _, job := range retryingJobs {
		// Transition from RETRYING â†’ QUEUED
		_, err := s.storeExt().TransitionJobState(
			job.ID,
			models.JobStatusQueued,
			fmt.Sprintf("Retry attempt %d/%d", job.RetryCount, s.config.RetryPolicy.MaxRetries),
		)

		if err != nil {
			log.Printf("[Cleanup] Failed to re-queue retrying job %s: %v", job.ID, err)
		}
	}
}

// getQueuedJobsPrioritized returns queued jobs with priority+fairness ordering
func (s *ProductionScheduler) getQueuedJobsPrioritized() ([]*models.Job, error) {
	queuedJobs, err := s.storeExt().GetJobsInState(models.JobStatusQueued)
	if err != nil {
		return nil, err
	}

	// Jobs are already ordered by creation time (FIFO within priority)
	// Apply aging to prevent starvation
	now := time.Now()
	for _, job := range queuedJobs {
		age := now.Sub(job.CreatedAt)

		// Aging factor: +1 priority level per 5 minutes
		agingBonus := int(age.Minutes() / 5)

		// Store aging bonus in a way that doesn't modify the job
		// (In practice, you'd use a priority score calculation)
		_ = agingBonus
	}

	return queuedJobs, nil
}

// getAvailableWorkers returns workers that can accept new jobs
func (s *ProductionScheduler) getAvailableWorkers() []*models.Node {
	allWorkers := s.store.GetAllNodes()
	available := []*models.Node{}

	for _, worker := range allWorkers {
		if worker.Status == "available" {
			available = append(available, worker)
		}
	}

	return available
}

// GetMetrics returns current scheduler metrics
func (s *ProductionScheduler) GetMetrics() *SchedulerMetrics {
	return s.metrics
}

// storeExt returns the store as ExtendedStore or panics
func (s *ProductionScheduler) storeExt() ExtendedStore {
	ext, ok := s.store.(ExtendedStore)
	if !ok {
		panic("Store does not implement ExtendedStore interface")
	}
	return ext
}

// rejectJob marks a job as rejected due to capability mismatch
func (s *ProductionScheduler) rejectJob(job *models.Job, reason string) {
	_, err := s.storeExt().TransitionJobState(
		job.ID,
		models.JobStatusRejected,
		reason,
	)

	if err != nil {
		log.Printf("[Scheduler] Failed to reject job %s: %v", job.ID, err)
		return
	}

	// Update failure reason in database
	if err := s.updateJobFailureReason(job.ID, models.FailureReasonCapabilityMismatch, reason); err != nil {
		log.Printf("[Scheduler] Failed to update failure reason for job %s: %v", job.ID, err)
	}

	log.Printf("[Scheduler] Job %d rejected: %s", job.SequenceNumber, reason)
}

// updateJobFailureReason updates the failure_reason field for a job
func (s *ProductionScheduler) updateJobFailureReason(jobID string, reason models.FailureReason, errorMsg string) error {
	// This is a direct SQL update - store interface doesn't have this method yet
	type sqlStore interface {
		UpdateJobFailureReason(jobID string, reason models.FailureReason, errorMsg string) error
	}
	
	if ss, ok := s.store.(sqlStore); ok {
		return ss.UpdateJobFailureReason(jobID, reason, errorMsg)
	}
	
	// Fallback: just log - failure reason will be empty but state is correct
	log.Printf("[Scheduler] Store doesn't support UpdateJobFailureReason, failure_reason not set")
	return nil
}

// removeWorker removes a worker from the list by ID
func removeWorker(workers []*models.Node, workerID string) []*models.Node {
	result := make([]*models.Node, 0, len(workers))
	for _, w := range workers {
		if w.ID != workerID {
			result = append(result, w)
		}
	}
	return result
}
